#!/usr/bin/env python
# coding=utf-8
"""
Usage: wiva <url> [ <revision> ]
            [ --all-articles [ --from=<from> ]]
            [ --wikitext=<wikitext_file> ]
            [ --debug ] [--json]
"""
import json
import logging
import urllib
import docopt
import requests
import requests.adapters
import time
import sys
from mediawiki import Article, Wiki
from validation import Validation
from wikitext_checkers import ALL_CHECKERS

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.CRITICAL)


def str2hex(s):
    return " ".join("0x{:02x}".format(ord(c)) for c in s)


def cli(arguments):
    requests.adapters.DEFAULT_RETRIES = 2

    if arguments['--debug']:
        logging.root.setLevel(logging.DEBUG)

    generate_json = arguments['--json']
    if arguments['--all-articles']:
        wiki = Wiki(arguments['<url>'])
        result = {}
        try:
            for processed_count, article in enumerate(wiki.iterarticles(start=arguments['--from'])):
                try:
                    article_result = process_article(article, processed_count, output='dict' if generate_json else 'plain')

                    if generate_json:
                        result.update(article_result)
                    else:
                        if article_result:
                            print article_result
                except UnicodeEncodeError:
                    continue
        except KeyboardInterrupt:
            pass
        finally:
            if generate_json:
                print pretty_json({wiki.url.host: result})
    else:

        if generate_json:
            article_dict = process_article(Article(arguments['<url>']), output='dict')
            print pretty_json(article_dict)
        else:
            print process_article(Article(arguments['<url>']))


def pretty_json(article_dict):
    return json.dumps(article_dict, sort_keys=True, indent=4, separators=(',', ': '))


def process_article(article, processed_count=None, output='plain'):
    if processed_count is not None:
        page_url = article.url.page
        text_msg = '[{:5d}] {} ...'.format(processed_count + 1, urllib.unquote(page_url))
        logger.info(text_msg)
        print >> sys.stderr, text_msg

    if arguments['--wikitext']:
        with open(arguments['--wikitext']) as f:
            article._wikitext = f.read()

    retries_left = 4
    wikitext = None
    while True:
        try:
            wikitext = article.wikitext
        except requests.ConnectionError:
            retries_left -= 1
            if retries_left <= 0:
                raise
            logger.debug('Request failed, sleeping for 3 seconds...')
            time.sleep(3)
        except (requests.exceptions.HTTPError, requests.exceptions.TooManyRedirects):
            return {} if output == 'dict' else ""
        else:
            break

    logger.info('  * wikitext length = {}'.format(len(wikitext)))

    validation = Validation(article, ALL_CHECKERS)

    if output == 'dict':
        json_dump = []
        for message in validation.messages:
            json_dump.append(dict(message))

        if json_dump:
            return {article.url.url: json_dump}
        else:
            return {}

    elif output == 'plain':

        return "\n".join(map(str, validation.messages))

    else:
        print >> sys.stderr, "Unsupported output: %s" % output
        raise


if __name__ == '__main__':
    arguments = docopt.docopt(__doc__)
    cli(arguments)

__author__ = 'alistra'
